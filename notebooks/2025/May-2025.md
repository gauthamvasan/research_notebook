### 2025-05-13, Tuesday

There are many topics that I'd like to dive deeper into. I'm further confused about who I am or who I should be or what I want to chase after. I feel like I've outgrown my teenage dreams and ideals, now lost in the wilderness, unable to figure out what it is that makes me tick. All I'm left with is fear about not being enough or not accomplishing enough in my lifetime. 

### 2025-05-14, Wednesday
Jesus, I didn't even finish an entry yestersay. I guess that's a quick reminder of the writer's block stemming from a chaotic flow of thoughts. 

**Warmstarting real-time reinforcement learning on robots using demonstrations**

The goal of my project at Sanctuary AI is to enable quick, sample-efficient RL on robots. Key requirements:
- Learn directly using robots -> real-time learning!!
- Use demonstrations to have a good starting point
- Continue learning using online RL, that is, real-time learning on robots

*Papers to read*
- [ ] [SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning](https://arxiv.org/pdf/2401.16013)
- [ ] [Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning](https://hil-serl.github.io/static/hil-serl-paper.pdf) --- a key competitor to keep in mind
- [ ] [Imitation Bootstrapped Reinforcement Learning](https://arxiv.org/pdf/2311.02198)
- [ ] [Accelerating Visual Sparse-Reward Learning with Latent Nearest-Demonstration-Guided Explorations](https://openreview.net/pdf?id=3NI5SxsJqf)
- [ ] [Efficient Online Reinforcement Learning with Offline Data](https://arxiv.org/pdf/2302.02948) --- RLPD is a key competitor, with simple, easy-to-implement ideas

*The paper I'm reading in detail and replication in the process*
- [ ] [Continuous Control with Coarse-to-fine Reinforcement Learning](https://younggyo.me/cqn/)

Rapid motor adaptation. Let's say we have limited demonstrations and a time budget of 50K steps. How do you learn fast? Is it even possible with a model-free method? What is the desired success rate? Can we beat CQN as flawed as the presented ideas are?

### 2025-05-16, Friday
The Coarse-to-Fine RL paper promises to be effective for real-time learning on robots with only access to limited demonstrations -  [[coarse-to-fine-q-learning]]

It has several design choices and key components, which I think are important for its effectiveness, least of it, the novelty in architecture

- Distributional critic inspired by C51
- Behavior cloning margin loss 
	- This is key!! Even though they claim their method is effective on sparse reward tasks, this is only a sleight of hand. The truly informative gradients are coming from the BC loss. The methods fails to learn when there's no BC loss --- this makes sense, since its hard to stumble upon the goal state due to the large action space
	- The whole idea is to force the Q-function to always prefer the actions provided by the expert demonstrations

It sounds like the issue faces by out-of-distribution (OOD) or over-estimation issues faced by offline learning methods in RL
	- Of course, we don't have full coverage of the state space 


